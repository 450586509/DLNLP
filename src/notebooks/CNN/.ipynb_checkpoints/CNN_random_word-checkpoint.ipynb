{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GT 630 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "from  os.path import join\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout,Activation, Lambda,Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D,Convolution2D\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8544\n",
      "2210\n",
      "1101\n",
      "[['a', 'stirring', ',', 'funny', 'and', 'finally', 'transport', 're-imagining', 'of', 'beauty', 'and', 'the', 'beast', 'and', '1930s', 'horror', 'film'], ['apparently', 'reassemble', 'from', 'the', 'cutting-room', 'floor', 'of', 'any', 'give', 'daytime', 'soap', '.']]\n",
      "[4, 1]\n"
     ]
    }
   ],
   "source": [
    "file_names = ['stsa.fine.test','stsa.fine.train','stsa.fine.dev']\n",
    "file_path = '/home/bruce/data/sentiment/citai_process'\n",
    "def read_file(fname=''):\n",
    "    with open(join(file_path,fname)) as fr:\n",
    "        lines = fr.readlines()\n",
    "    lines = [line.strip().lower() for line in lines]\n",
    "    lables = [int(line[0:1]) for line in lines]\n",
    "    words = [line[2:].split() for line in lines]\n",
    "    return words,lables       \n",
    "train_X,train_y = read_file(fname='stsa.fine.train')\n",
    "test_X,test_y = read_file(fname='stsa.fine.test')\n",
    "dev_X,dev_y = read_file(fname='stsa.fine.dev')\n",
    "print(len(train_X))\n",
    "print(len(test_X))\n",
    "print(len(dev_X))\n",
    "print(train_X[0:2])\n",
    "print(train_y[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 句子长度统计信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length =  8544\n",
      "max =  55\n",
      "min = 2\n",
      "average =  19.579470973782772\n",
      "top 50% =  19\n",
      "top 80% =  28\n",
      "top 90% =  33\n",
      "top 95% =  36\n"
     ]
    }
   ],
   "source": [
    "def statics_list2(arrays=[]):\n",
    "    lengths = [len(i) for i in arrays]\n",
    "    lengths = sorted(lengths)\n",
    "    length = len(lengths)\n",
    "    print('length = ',len(lengths))\n",
    "    print('max = ',lengths[-1])\n",
    "    print('min =',lengths[0])\n",
    "    print('average = ',sum(lengths)/length)\n",
    "    print('top 50% = ',lengths[int(0.5*length)])\n",
    "    print('top 80% = ',lengths[int(0.8*length)])\n",
    "    print('top 90% = ',lengths[int(0.9*length)])\n",
    "    print('top 95% = ',lengths[int(0.95*length)])\n",
    "    \n",
    "statics_list2(arrays=train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_to_index(datas=[]):\n",
    "    word_index={}\n",
    "    count=1\n",
    "    for data in datas:\n",
    "        for list_ in data:\n",
    "            for w in list_:\n",
    "                if w not in word_index:\n",
    "                    word_index[w] = count\n",
    "                    count = count + 1\n",
    "    print('leng of word_index =',len(word_index))\n",
    "    for i in range(len(datas)):\n",
    "        datas[i] = [[ word_index[w] for w in line ] for line in datas[i]] \n",
    "    return datas,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leng of word_index = 14497\n",
      "14497\n"
     ]
    }
   ],
   "source": [
    "X,word_index = token_to_index(datas=[train_X,dev_X])\n",
    "train_X,dev_X = X\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 36\n",
    "batch_size=32\n",
    "\n",
    "max_features = 14498\n",
    "embedding_dims = 250\n",
    "\n",
    "nb_filter = 150\n",
    "filter_length = 2\n",
    "dense1_hindden = 150\n",
    "nb_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "finish build\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features,\n",
    "                    output_dim = embedding_dims\n",
    "                   ))\n",
    "model.add(Convolution1D(nb_filter = nb_filter,\n",
    "                        filter_length = filter_length,\n",
    "                        border_mode = 'valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length = 1\n",
    "                       ))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(dense1_hindden))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = 'adadelta',\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "print('finish build')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 模型图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_generator(X=None,y=None):\n",
    "    i = 0\n",
    "    max_i = int(len(X)/batch_size)\n",
    "    while True:\n",
    "        i = i % max_i\n",
    "        x_batch = X[i*batch_size:(i+1)*batch_size]\n",
    "        y_batch = y[i*batch_size:(i+1)*batch_size]\n",
    "        yield (x_batch,y_batch)\n",
    "        i = i + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3200/3200 [==============================] - 7s - loss: 1.3270 - acc: 0.4428 - val_loss: 1.4102 - val_acc: 0.3815\n",
      "Epoch 2/100\n",
      "3200/3200 [==============================] - 7s - loss: 1.3314 - acc: 0.4347 - val_loss: 1.3897 - val_acc: 0.3960\n",
      "Epoch 3/100\n",
      "3200/3200 [==============================] - 7s - loss: 1.2626 - acc: 0.4738 - val_loss: 1.3828 - val_acc: 0.3996\n",
      "Epoch 4/100\n",
      "3072/3200 [===========================>..] - ETA: 0s - loss: 1.2142 - acc: 0.4945"
     ]
    }
   ],
   "source": [
    "model.fit_generator(my_generator(train_X_model,train_y_model),samples_per_epoch = 32*100,nb_epoch=100,verbose=1,validation_data=(dev_X_model,dev_y_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 试验记录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2016年11月12日 10：16  最佳 0.4015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "参数\n",
    "\n",
    "max_len = 36 batch_size=32\n",
    "\n",
    "max_features = 14714 embedding_dims = 100\n",
    "\n",
    "nb_filter = 150  filter_length = 2  dense1_hindden = 100  nb_classes = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2016年11月12日 10：22 最佳成绩 0.4069 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "参数\n",
    "\n",
    "max_len = 36 batch_size=32\n",
    "\n",
    "max_features = 14714 embedding_dims = 50\n",
    "\n",
    "nb_filter = 150  filter_length = 2  dense1_hindden = 100  nb_classes = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2016年11月12日 10：22 最佳成绩  0.4151"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数\n",
    "\n",
    "max_len = 36 batch_size=32\n",
    "\n",
    "max_features = 14714 embedding_dims = 150\n",
    "\n",
    "nb_filter = 150 filter_length = 2 dense1_hindden = 100 nb_classes = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2016年11月12日 10：22 最佳成绩  0.4242 [ 0.4214, 0.4033,0.4024 ,0.4151,0.4242]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数\n",
    "\n",
    "max_len = 36 batch_size=32\n",
    "\n",
    "max_features = 14714 embedding_dims = 200\n",
    "\n",
    "nb_filter = 150 filter_length = 2 dense1_hindden = 100 nb_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
